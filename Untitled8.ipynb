{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14b12cb3-baaa-406d-9d5c-1fcef398ac30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started load_multiple_gedi_files\n",
      "Found 58 files matching the prefix.\n",
      "Processing file: test-agb-bucket/GEDIL4A2023/GEDI04_A_2023060022721_O23871_01_T01222_02_003_01_V002.h5\n",
      "ANCILLARY\n",
      "BEAM0000\n",
      "BEAM0001\n",
      "BEAM0010\n",
      "BEAM0011\n",
      "BEAM0101\n",
      "BEAM0110\n",
      "BEAM1000\n",
      "BEAM1011\n",
      "METADATA\n",
      "xarray\n",
      "Error loading GEDI files: 'GCSFile' object has no attribute 'seek'\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import dask\n",
    "import dask.array as da\n",
    "import xarray as xr\n",
    "import gcsfs\n",
    "import geemap\n",
    "import ee\n",
    "import numpy as np\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from dask_ml.metrics import mean_squared_error\n",
    "from dask import delayed\n",
    "from rasterio.transform import from_origin\n",
    "import rasterio\n",
    "import h5py\n",
    "import xarray as xr\n",
    "import gcsfs\n",
    "import subprocess\n",
    "import netCDF4 as nc4\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import gcsfs\n",
    "\n",
    "# Initialize GCS filesystem\n",
    "fs = gcsfs.GCSFileSystem(project='your-project-id')\n",
    "# Initialize Google Earth Engine\n",
    "ee.Initialize()\n",
    "\n",
    "# GCS File System Setup\n",
    "fs = gcsfs.GCSFileSystem(project='test-agb-bucket')  # Replace with your GCP project ID\n",
    "pfile = 'b.h5'\n",
    "\n",
    "# Function to load and inspect the GEDI file\n",
    "def load_and_print_gedi_data(gcs_file_path):\n",
    "    try:\n",
    "        with fs.open(gcs_file_path, 'rb') as f:\n",
    "\n",
    "            # Open the file using h5py\n",
    "            with h5py.File(f, 'r') as hdf:\n",
    "                # Print the structure of the HDF5 file\n",
    "                # print(\"HDF5 file structure:\")\n",
    "                # hdf.visititems(lambda name, obj: print(f\"{name}: {obj}\"))\n",
    "                # Optionally, print the available keys (datasets)\n",
    "                # print(\"Keys in HDF5 file:\", list(hdf.keys()))\n",
    "                \n",
    "                # Explore the dataset, print all available attributes and datasets\n",
    "                for key in hdf.keys():\n",
    "                    print(f\"Key: {key}\")\n",
    "                    group = hdf[key]\n",
    "                    print(f\"  Group/Dataset {key} contains: {list(group.keys())}\")\n",
    "                    print(f\"  Attributes: {group.attrs}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error inspecting GEDI file: {e}\")\n",
    "\n",
    "# Function to load multiple GEDI files from GCS with filtering\n",
    "def load_multiple_gedi_files(gcs_prefix):\n",
    "    try:\n",
    "        print(\"Started load_multiple_gedi_files\")\n",
    "        # List all files that match the prefix in the GCS bucket\n",
    "        gcs_files = fs.glob(gcs_prefix + \"*.h5\")\n",
    "        if not gcs_files:\n",
    "            raise FileNotFoundError(f\"No files found with prefix {gcs_prefix}.\")\n",
    "        print(f\"Found {len(gcs_files)} files matching the prefix.\")\n",
    "        \n",
    "        # Load and concatenate all files\n",
    "        datasets = []\n",
    "        for file in gcs_files:\n",
    "            print(f\"Processing file: {file}\")\n",
    "            # Read the content or process the file here\n",
    "            data = file.read()\n",
    "            # load_and_print_gedi_data(file)\n",
    "            with fs.open(pfile, 'rb') as f:\n",
    "                # Open the file using h5py to extract relevant data\n",
    "                with h5py.File(f, 'x') as hdf5file:\n",
    "                    # List all groups (keys) in the HDF5 file\n",
    "                    for key in hdf5file.keys():\n",
    "                        print(key)\n",
    "\n",
    "                    # Access the relevant dataset, e.g., 'beam0001' for a specific beam's data\n",
    "                    # data = hdf5file['BEAM0000']  # Adjust the beam key depending on your data\n",
    "                    # Access the /BEAM0000 group\n",
    "                    beam_group = hdf5file['BEAM0000']\n",
    "                    \n",
    "                    # Prepare a dictionary to store datasets for the xarray Dataset\n",
    "                    data_dict = {}\n",
    "                    \n",
    "                    # Loop through the members of the group\n",
    "                    for name, dataset in beam_group.items():\n",
    "                        if name in ['agbd','l4_quality_flag', 'lat_lowestmode', 'lon_lowestmode']:\n",
    "                            # Assume that the dataset is a numpy array\n",
    "                            data_dict[name] = dataset[:]\n",
    "                    \n",
    "                    # Convert the dictionary into an xarray Dataset\n",
    "                    xarray_data = xr.Dataset(data_dict)\n",
    "                    print(\"xarray\")\n",
    "                datasets.append(xarray_data)\n",
    "                # Now you have the data from '/BEAM0000' in an xarray.Dataset\n",
    "                    # print(data.items())\n",
    "                    # # Print the structure of the dataset to see what variables are available\n",
    "                    # print(data)\n",
    "                    # # # Open the GEDI .h5 file as a NetCDF-compatible file\n",
    "                    # # ds = xr.open_dataset(data, engine='h5netcdf')\n",
    "                    # # print(ds)\n",
    "\n",
    "\n",
    "\n",
    "                    # # Convert the dataset into a DataFrame (assuming the dataset is in a tabular form)\n",
    "                    # df = pd.DataFrame(data[:])\n",
    "                    \n",
    "                    # # Save to CSV\n",
    "                    # csv_file = 'output.csv'\n",
    "                    # df.to_csv(csv_file, index=False)\n",
    "                \n",
    "                    # print(f\"CSV file saved as {csv_file}\")\n",
    "            #         # Access necessary datasets (assuming the variables are named 'agbd' and 'l4_flag')\n",
    "            #         if 'agbd' in list(data.keys()) and 'l4_quality_flag' in list(data.keys()):  \n",
    "                        \n",
    "            #             agbd_data = data['agbd'][:]\n",
    "            #             print(len(agbd_data))\n",
    "            #             l4_flag_data = data['l4_quality_flag'][:]\n",
    "            #             latitudes = data['lat_lowestmode'][:]\n",
    "            #             longitudes = data['lon_lowestmode'][:]\n",
    "            #             print(\"checkpoint -1\")\n",
    "            #             # Create an xarray Dataset from the data\n",
    "            #             # dataset = xr.Dataset({\n",
    "            #             #     'agbd': (['lat', 'lon'], agbd_data),\n",
    "            #             #     'l4_quality_flag': (['lat', 'lon'], l4_flag_data)\n",
    "            #             # }, coords={\n",
    "            #             #            'lat' : lat_lowestmode,\n",
    "            #             #            'lon' : lon_lowestmode\n",
    "            #             # })\n",
    "\n",
    "            #             # Ensure lat and lon have unique values if gridding is required\n",
    "            #             lat_bins = np.linspace(latitudes.min(), latitudes.max(), 100)  # Adjust number of bins\n",
    "            #             lon_bins = np.linspace(longitudes.min(), longitudes.max(), 100)\n",
    "                        \n",
    "            #             # Create a 2D grid if necessary\n",
    "            #             lat_idx = np.digitize(latitudes, lat_bins) - 1\n",
    "            #             lon_idx = np.digitize(longitudes, lon_bins) - 1\n",
    "                        \n",
    "            #             # Initialize a 2D array for gridded data\n",
    "            #             data_grid1 = np.full((len(lat_bins)-1, len(lon_bins)-1), fill_value=np.nan, dtype=np.float32)\n",
    "            #             # Initialize a 2D array for gridded data\n",
    "            #             data_grid2 = np.full((len(lat_bins)-1, len(lon_bins)-1), fill_value=np.nan, dtype=np.int32)\n",
    "                        \n",
    "            #             # Populate the 2D grid with data\n",
    "            #             for val, lat, lon in zip(agbd_data, lat_idx, lon_idx):\n",
    "            #                 if 0 <= lat < data_grid1.shape[0] and 0 <= lon < data_grid1.shape[1]:\n",
    "            #                     data_grid1[lat, lon] = val\n",
    "\n",
    "            #             # Populate the 2D grid with data\n",
    "            #             for val, lat, lon in zip(l4_flag_data, lat_idx, lon_idx):\n",
    "            #                 if 0 <= lat < data_grid2.shape[0] and 0 <= lon < data_grid2.shape[1]:\n",
    "            #                     data_grid2[lat, lon] = val\n",
    "                        \n",
    "            #             # Compute bin midpoints for latitude and longitude\n",
    "            #             lat_coords = (lat_bins[:-1] + lat_bins[1:]) / 2\n",
    "            #             lon_coords = (lon_bins[:-1] + lon_bins[1:]) / 2\n",
    "\n",
    "\n",
    "            #             # Create the xarray Dataset\n",
    "            #             dataset = xr.Dataset(\n",
    "            #                 {\n",
    "            #                     \"agbd\": ([\"lat\", \"lon\"], data_grid1),  # Variable with lat and lon dimensions\n",
    "            #                     \"l4_quality_flag\": ([\"lat\", \"lon\"], data_grid2)\n",
    "            #                 },\n",
    "            #                 coords={\n",
    "            #                     \"lat\": lat_coords,  # Latitude coordinates\n",
    "            #                     \"lon\": lon_coords   # Longitude coordinates\n",
    "            #                 }\n",
    "            #             )\n",
    "            #             # dataset = xr.Dataset(\n",
    "            #             #     data_vars=dict(\n",
    "            #             #         agbd=(['lat', 'lon'], agbd_data),\n",
    "            #             #         l4_quality_flag=(['lat', 'lon'], l4_flag_data),\n",
    "            #             #     ),\n",
    "            #             #     coords=dict(\n",
    "            #             #         lon=('lon', lon_lowestmode),\n",
    "            #             #         lat=('lat', lat_lowestmode)\n",
    "            #             #     ),\n",
    "            #             #     attrs=dict(description=\"agb related data.\"),\n",
    "            #             # )\n",
    "                        \n",
    "            #             print(\"checkpoint 0\", dataset)\n",
    "            #             datasets.append(dataset)\n",
    "            #             print(\"checkpoint 1\")\n",
    "            #         else:\n",
    "            #             print(f\"Skipping file {file}, missing 'agbd' or 'l4_quality_flag'.\")\n",
    "\n",
    "        print(\"checkpoint 2\")\n",
    "        # Concatenate all datasets along the 'time' dimension\n",
    "        if datasets:\n",
    "            combined_data = xr.concat(datasets)\n",
    "            \n",
    "            # Filter data (AGBD not null and l4_flag == 1)\n",
    "            filtered_data = combined_data.where((combined_data['agbd'] > 0) & (combined_data['l4_quality_flag'] == 1), drop=True)\n",
    "            print(f\"Filtered data: {filtered_data}\")\n",
    "            return filtered_data\n",
    "        else:\n",
    "            print(\"No valid datasets found.\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading GEDI files: {e}\")\n",
    "        return None\n",
    "\n",
    "# Fetch Satellite Data from GEE (Sentinel-1, Sentinel-2, Landsat)\n",
    "def fetch_gee_data(region, start_date, end_date):\n",
    "    try:\n",
    "        print(\"Started fetch_gee_data\")\n",
    "        # Sentinel-1 VV and VH\n",
    "        sentinel1_vv_vh = ee.ImageCollection('COPERNICUS/S1_GRD') \\\n",
    "            .filterBounds(region) \\\n",
    "            .filterDate(start_date, end_date) \\\n",
    "            .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV')) \\\n",
    "            .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VH'))\n",
    "        \n",
    "        # Sentinel-2 NDVI, EVI\n",
    "        sentinel2 = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \\\n",
    "            .filterBounds(region) \\\n",
    "            .filterDate(start_date, end_date) \\\n",
    "            .select(['B4', 'B8', 'B3', 'B2'])\n",
    "\n",
    "        # Landsat DEM (slope, aspect)\n",
    "        landsat_dem = ee.ImageCollection('USGS/SRTMGL1_003') \\\n",
    "            .filterBounds(region) \\\n",
    "            .filterDate(start_date, end_date)\n",
    "\n",
    "\n",
    "        # Load Landsat 8 Surface Reflectance data\n",
    "        landsat8 = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2') \\\n",
    "                      .filterBounds(region) \\\n",
    "                      .filterDate(start_date, end_date) \\\n",
    "                      .filter(ee.Filter.lt('CLOUD_COVER', 50))\n",
    "\n",
    "\n",
    "        # Load the GLO-30 DEM data from the COPERNICUS collection\n",
    "        dem = ee.ImageCollection('COPERNICUS/DEM/GLO30') \\\n",
    "                  .filterBounds(region) \\\n",
    "                  .mosaic()\n",
    "\n",
    "        return sentinel1_vv_vh, sentinel2, landsat8, dem\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching satellite data: {e}\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "# Calculate NDVI, EVI, Slope, Aspect from data\n",
    "def process_satellite_data(sentinel1, sentinel2, landsat8, dem):\n",
    "    try:\n",
    "        print(\"Started process_satellite_data\")\n",
    "        # Calculate NDVI and EVI for Sentinel-2\n",
    "        def calc_ndvi(image):\n",
    "            return image.normalizedDifference(['B8', 'B4']).rename('NDVI')\n",
    "        \n",
    "        def calc_evi(image):\n",
    "            return image.expression(\n",
    "                '2.5 * ((B8 - B4) / (B8 + 6 * B4 - 7.5 * B2 + 1))',\n",
    "                {\n",
    "                    'B8': image.select('B8'),\n",
    "                    'B4': image.select('B4'),\n",
    "                    'B2': image.select('B2')\n",
    "                }\n",
    "            ).rename('EVI')\n",
    "        \n",
    "        sentinel2_ndvi = sentinel2.map(calc_ndvi)\n",
    "        sentinel2_evi = sentinel2.map(calc_evi)\n",
    "\n",
    "        # Calculate NDVI for Landsat 8\n",
    "        landsat_ndvi = landsat8.map(lambda image: image.normalizedDifference(['SR_B5', 'SR_B4']).rename('NDVI')).median()\n",
    "\n",
    "        # Calculate slope in degrees\n",
    "        slope = ee.Terrain.slope(dem)\n",
    "        # Calculate aspect in degrees\n",
    "        aspect = ee.Terrain.aspect(dem)\n",
    "\n",
    "        # Sentinel-1 VV/VH (for radar backscatter)\n",
    "        sentinel1_vv = sentinel1.select('VV')\n",
    "        sentinel1_vh = sentinel1.select('VH')\n",
    "        print(\"Finished process_satellite_data\")\n",
    "        return sentinel2_ndvi, sentinel2_evi, landsat_ndvi, slope, aspect, sentinel1_vv, sentinel1_vh\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing satellite data: {e}\")\n",
    "        return None, None, None, None, None, None, None\n",
    "\n",
    "# Prepare ML Model Input\n",
    "def prepare_ml_input( sentinel2_ndvi, sentinel2_evi, landsat_ndvi, slope, aspect, sentinel1_vv, sentinel1_vh):\n",
    "    try:\n",
    "        print(\"Started prepare_ml_input\")\n",
    "        # Align and merge datasets into a single Xarray Dataset\n",
    "        merged = xr.Dataset({\n",
    "            'NDVI': sentinel2_ndvi,\n",
    "            'EVI': sentinel2_evi,\n",
    "            'Landsat_ndvi' : landsat_ndvi,\n",
    "            'Slope': slope,\n",
    "            'Aspect': aspect,\n",
    "            'VV': sentinel1_vv.mean(),\n",
    "            'VH': sentinel1_vh.mean()\n",
    "        })\n",
    "        \n",
    "        # Using Dask for parallelization and optimization\n",
    "        merged = merged.chunk({'time': 100, 'lat': 500, 'lon': 500})\n",
    "        return merged\n",
    "    except Exception as e:\n",
    "        print(f\"Error preparing ML input: {e}\")\n",
    "        return None\n",
    "\n",
    "# Machine Learning Model - RandomForest with Dask (using sklearn and Dask for parallelism)\n",
    "def train_model(input_data):\n",
    "    try:\n",
    "        print(\"Started train_model\")\n",
    "        # Convert to Dask-backed arrays\n",
    "        X = input_data[['NDVI', 'EVI', 'Landsat_ndvi', 'Slope', 'Aspect', 'VV', 'VH']].values\n",
    "        y = input_data['AGBD'].values\n",
    "        \n",
    "        # Dask parallelized train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Initialize a RandomForest model (sklearn's RandomForestRegressor)\n",
    "        rf_model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "        \n",
    "        # Train the model on the training set (using Dask parallelism)\n",
    "        rf_model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on the test set\n",
    "        y_pred = rf_model.predict(X_test)\n",
    "        \n",
    "        # Calculate performance (mean squared error and RMSE)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse.compute())  # Calculate RMSE by taking square root of MSE\n",
    "        print(f\"Model Mean Squared Error: {mse.compute()}\")\n",
    "        print(f\"Model Root Mean Squared Error (RMSE): {rmse}\")\n",
    "        \n",
    "        return rf_model\n",
    "    except Exception as e:\n",
    "        print(f\"Error training model: {e}\")\n",
    "        return None\n",
    "\n",
    "# Predict AGB Map\n",
    "def predict_agb(model, input_data):\n",
    "    try:\n",
    "        print(\"Started predict_agb\")\n",
    "        # Placeholder for prediction logic\n",
    "        X = input_data[['NDVI', 'EVI', 'Landsat_ndvi', 'Slope', 'Aspect', 'VV', 'VH']].values\n",
    "        predicted_agb = model.predict(X)\n",
    "        return predicted_agb\n",
    "    except Exception as e:\n",
    "        print(f\"Error predicting AGB map: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to save the AGB map as a NetCDF file\n",
    "def save_as_netcdf(agb_map, output_filename):\n",
    "    try:\n",
    "        # Check if agb_map is an Xarray DataArray\n",
    "        if isinstance(agb_map, xr.DataArray):\n",
    "            # Save the DataArray as a NetCDF file\n",
    "            agb_map.to_netcdf(output_filename)\n",
    "            print(f\"AGB map saved as {output_filename}\")\n",
    "        else:\n",
    "            print(\"The agb_map is not an Xarray DataArray.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving AGB map as NetCDF: {e}\")\n",
    "\n",
    "# Main function to integrate everything\n",
    "def main(gcs_gedi_prefix, region, start_date, end_date):\n",
    "    try:\n",
    "        # Step 1: Load GEDI data (multiple files)\n",
    "        gedi_data = load_multiple_gedi_files(gcs_gedi_prefix)\n",
    "        if gedi_data is None:\n",
    "            return None\n",
    "        \n",
    "        # Step 2: Fetch satellite data from GEE\n",
    "        sentinel1_vv_vh, sentinel2, landsat8, dem = fetch_gee_data(region, start_date, end_date)\n",
    "        if None in [sentinel1_vv_vh, sentinel2, landsat8, dem]:\n",
    "            return None\n",
    "        \n",
    "        # Step 3: Process satellite data\n",
    "        sentinel2_ndvi, sentinel2_evi, landsat_ndvi, slope, aspect, sentinel1_vv, sentinel1_vh = process_satellite_data(sentinel1_vv_vh, sentinel2, landsat8, dem)\n",
    "        if None in [sentinel2_ndvi, sentinel2_evi, landsat_ndvi, slope, aspect, sentinel1_vv, sentinel1_vh]:\n",
    "            return None\n",
    "        \n",
    "        # Step 4: Prepare ML input data\n",
    "        ml_input_data = prepare_ml_input( sentinel2_ndvi, sentinel2_evi, landsat_ndvi, slope, aspect, sentinel1_vv, sentinel1_vh)\n",
    "        if ml_input_data is None:\n",
    "            return None\n",
    "        \n",
    "        # Step 5: Train model\n",
    "        model = train_model(ml_input_data)\n",
    "        if model is None:\n",
    "            return None\n",
    "        \n",
    "        # Step 6: Predict AGB map\n",
    "        agb_map = predict_agb(model, ml_input_data)\n",
    "        if agb_map is None:\n",
    "            return None\n",
    "        \n",
    "        # Save the AGB map as NetCDF\n",
    "        save_as_netcdf(agb_map, 'agb_map.nc')\n",
    "        \n",
    "        return agb_map\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main process: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example Usage\n",
    "gcs_gedi_prefix = 'gs://test-agb-bucket/GEDIL4A2023/GEDI'  # GCS prefix for multiple files\n",
    "region = ee.Geometry.Polygon([[-75.0, -5.0], [-70.0, -5.0], [-70.0, 0.0], [-75.0, 0.0]])  # Example region in Amazon Basin\n",
    "start_date = '2023-03-01'\n",
    "end_date = '2023-08-31'\n",
    "\n",
    "agb_map = main(gcs_gedi_prefix, region, start_date, end_date)\n",
    "\n",
    "# Output the AGB map\n",
    "print(agb_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2092567-c5e7-4c0d-990e-7ce90095de77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
